{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#한글용 오픈소스 초거대 언어 모델(Open-Source LLM) 예제 사례\n",
        "\n",
        "### 작성자: 서진호(jinhoseo@stud.assist.ac.kr)\n",
        "\n",
        "##### 1. 한글 모델은 이준범님이 공개한 KoAlpaca-Polyglot-12.8B 사용.\n",
        "##### 2. 주의사항: 반드시 CoPro (유료) 와 Spot Instance 컴퓨팅 단위($9.9) 가입하셔야 코랩에서 NVIDIA A100 사용해서 GPU하고 모델 실행할 수 있음.\n",
        "##### 3. 대부분 발생하는 에러는 세션이 끊어져서 발생함. 구글 코랩은 전체 사용자의 자원 관리하므로 특정 사용자가 자원을 많이 사용하면 강제로 끊을 수 있음을 유의하기 바람.\n",
        "##### 4. 전체 다운로드 받는데 빠르면 10분 늦으면 30분 정도 걸림.\n",
        "##### 5. PyTorch 언어를 사용하고, 8bit로 quantization, 그리고 gradio로 HTML UI 사용"
      ],
      "metadata": {
        "id": "zcms8Hj7c0FS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kem7h0FGxbcZ"
      },
      "outputs": [],
      "source": [
        "#오픈소스 실행 전에 반드시 관련 요구 컴포넌트 설치\n",
        "\n",
        "!pip install -q gradio torch transformers accelerate bitsandbytes Xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "\n",
        "MODEL = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    revision=\"8bit\",\n",
        "    # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=MODEL,\n",
        "    # device=2,\n",
        ")\n",
        "\n",
        "\n",
        "def answer(state, state_chatbot, text):\n",
        "    messages = state + [{\"role\": \"질문\", \"content\": text}]\n",
        "\n",
        "    conversation_history = \"\\n\".join(\n",
        "        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n",
        "    )\n",
        "\n",
        "    ans = pipe(\n",
        "        conversation_history + \"\\n\\n### 답변:\",\n",
        "        do_sample=True,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=2\n",
        "    )\n",
        "\n",
        "    msg = ans[0][\"generated_text\"]\n",
        "\n",
        "    if \"###\" in msg:\n",
        "        msg = msg.split(\"###\")[0]\n",
        "\n",
        "    new_state = [{\"role\": \"이전 질문\", \"content\": text}, {\"role\": \"이전 답변\", \"content\": msg}]\n",
        "\n",
        "    state = state + new_state\n",
        "    state_chatbot = state_chatbot + [(text, msg)]\n",
        "\n",
        "    print(state)\n",
        "    print(state_chatbot)\n",
        "\n",
        "    return state, state_chatbot, state_chatbot\n",
        "\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:750px}\") as demo:\n",
        "    state = gr.State(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"맥락\",\n",
        "                \"content\": \"KoAlpaca는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자인 이준범인 개발한 모델을 이용했습니다.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"맥락\",\n",
        "                \"content\": \"ChatKoAlpaca는 KoAlpaca를 채팅 유형으로 만든 봇입니다.\",\n",
        "            },\n",
        "            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n",
        "            {\n",
        "                \"role\": \"명령어\",\n",
        "                \"content\": \"인사에는 짧고 간단히 친절하게 인사로 답하고, 아래 대화에 간단하고 짧게 답해줍니다.\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "    state_chatbot = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.HTML(\n",
        "            \"\"\"<div style=\"text-align: center; max-width: 500px; margin: 0 auto;\">\n",
        "            <div>\n",
        "                <h1>ChatKoAlpaca 12.8B (v1.1b-chat-8bit)</h1>\n",
        "            </div>\n",
        "            <div>\n",
        "                Demo runs on CoPro and Spot Instance(A100) with 8bit quantized\n",
        "            </div>\n",
        "        </div>\"\"\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"메시지를 적어주세요...\").style(\n",
        "            container=False\n",
        "        )\n",
        "\n",
        "    txt.submit(answer, [state, state_chatbot, txt], [state, state_chatbot, chatbot])\n",
        "    txt.submit(lambda: \"\", None, txt)\n",
        "\n",
        "demo.launch(debug=True, server_name=\"0.0.0.0\")\n"
      ],
      "metadata": {
        "id": "72nLx0ywy66M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilQ5MDT9cysv"
      }
    }
  ]
}