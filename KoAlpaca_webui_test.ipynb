{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kem7h0FGxbcZ"
      },
      "outputs": [],
      "source": [
        "!pip install gradio torch transformers accelerate bitsandbytes Xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM\n",
        "\n",
        "MODEL = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    revision=\"8bit\",\n",
        "    # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=MODEL,\n",
        "    # device=2,\n",
        ")\n",
        "\n",
        "\n",
        "def answer(state, state_chatbot, text):\n",
        "    messages = state + [{\"role\": \"질문\", \"content\": text}]\n",
        "\n",
        "    conversation_history = \"\\n\".join(\n",
        "        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n",
        "    )\n",
        "\n",
        "    ans = pipe(\n",
        "        conversation_history + \"\\n\\n### 답변:\",\n",
        "        do_sample=True,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "\n",
        "    msg = ans[0][\"generated_text\"]\n",
        "\n",
        "    if \"###\" in msg:\n",
        "        msg = msg.split(\"###\")[0]\n",
        "\n",
        "    new_state = [{\"role\": \"이전 질문\", \"content\": text}, {\"role\": \"이전 답변\", \"content\": msg}]\n",
        "\n",
        "    state = state + new_state\n",
        "    state_chatbot = state_chatbot + [(text, msg)]\n",
        "\n",
        "    print(state)\n",
        "    print(state_chatbot)\n",
        "\n",
        "    return state, state_chatbot, state_chatbot\n",
        "\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:750px}\") as demo:\n",
        "    state = gr.State(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"맥락\",\n",
        "                \"content\": \"KoAlpaca(코알파카)는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자 Beomi가 개발한 모델입니다.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"맥락\",\n",
        "                \"content\": \"ChatKoAlpaca(챗코알파카)는 KoAlpaca를 채팅형으로 만든 것입니다.\",\n",
        "            },\n",
        "            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n",
        "            {\n",
        "                \"role\": \"명령어\",\n",
        "                \"content\": \"인사에는 짧고 간단한 친절한 인사로 답하고, 아래 대화에 간단하고 짧게 답해주세요.\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "    state_chatbot = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.HTML(\n",
        "            \"\"\"<div style=\"text-align: center; max-width: 500px; margin: 0 auto;\">\n",
        "            <div>\n",
        "                <h1>ChatKoAlpaca 12.8B (v1.1b-chat-8bit)</h1>\n",
        "            </div>\n",
        "            <div>\n",
        "                Demo runs on RTX 3090 (24GB) with 8bit quantized\n",
        "            </div>\n",
        "        </div>\"\"\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Send a message...\").style(\n",
        "            container=False\n",
        "        )\n",
        "\n",
        "    txt.submit(answer, [state, state_chatbot, txt], [state, state_chatbot, chatbot])\n",
        "    txt.submit(lambda: \"\", None, txt)\n",
        "\n",
        "demo.launch(debug=True, server_name=\"0.0.0.0\")\n"
      ],
      "metadata": {
        "id": "72nLx0ywy66M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}