{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPGtuXlXyaqp"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install loralib\n",
        "!pip install torch torchvision torchaudio tokenizers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\")"
      ],
      "metadata": {
        "id": "coyw2O6wYb3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM\n",
        "\n",
        "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(device=f\"cuda\", non_blocking=True)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation', \n",
        "    model=model,\n",
        "    tokenizer=MODEL,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "def ask(x, context='', is_input_full=False):\n",
        "    ans = pipe(\n",
        "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\", \n",
        "        do_sample=True, \n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(ans[0]['generated_text'])\n",
        "\n",
        "ask(\"딥러닝이 뭐야?\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N0VqMCEGzALU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}